{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoimport wherept.py:\n",
    "%load_ext autoreload\n",
    "%aimport wherept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10f46b170>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlage\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/lage/Development/woher/gen3/wandb/run-20231202_212514-gmfk1etm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lage/woher/runs/gmfk1etm' target=\"_blank\">still-haze-18</a></strong> to <a href='https://wandb.ai/lage/woher' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lage/woher' target=\"_blank\">https://wandb.ai/lage/woher</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lage/woher/runs/gmfk1etm' target=\"_blank\">https://wandb.ai/lage/woher/runs/gmfk1etm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"woher\", job_type=\"transformer-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>asciiname</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>country_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Soldeu</td>\n",
       "      <td>Soldeu</td>\n",
       "      <td>42.57688</td>\n",
       "      <td>1.66769</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El Tarter</td>\n",
       "      <td>El Tarter</td>\n",
       "      <td>42.57952</td>\n",
       "      <td>1.65362</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sant Julià de Lòria</td>\n",
       "      <td>Sant Julia de Loria</td>\n",
       "      <td>42.46372</td>\n",
       "      <td>1.49129</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pas de la Casa</td>\n",
       "      <td>Pas de la Casa</td>\n",
       "      <td>42.54277</td>\n",
       "      <td>1.73361</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ordino</td>\n",
       "      <td>Ordino</td>\n",
       "      <td>42.55623</td>\n",
       "      <td>1.53319</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name            asciiname  latitude  longitude country_code\n",
       "0               Soldeu               Soldeu  42.57688    1.66769           AD\n",
       "1            El Tarter            El Tarter  42.57952    1.65362           AD\n",
       "2  Sant Julià de Lòria  Sant Julia de Loria  42.46372    1.49129           AD\n",
       "3       Pas de la Casa       Pas de la Casa  42.54277    1.73361           AD\n",
       "4               Ordino               Ordino  42.55623    1.53319           AD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = run.use_artifact(\"woher/cleaned-cities:latest\").get(\"clean\")\n",
    "df_raw = dataset.get_dataframe()\n",
    "df_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "TARGET_COL = \"asciiname\"\n",
    "START_CHAR = \"<\"\n",
    "END_CHAR = \">\"\n",
    "PADDING_CHAR = \"#\"\n",
    "BATCH_SIZE = 16\n",
    "BLOCK_SIZE = 8\n",
    "N_EMBED = 128\n",
    "N_HEADS = 4\n",
    "N_LAYER = 2\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 61\n",
      "Vocabulary:  #'-.1<>ABCDEFGHIJKLMNOPQRSTUVWXYZ`abcdefghijklmnopqrstuvwxyz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>asciiname</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>country_code</th>\n",
       "      <th>target_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Soldeu</td>\n",
       "      <td>&lt;Soldeu&gt;######################################...</td>\n",
       "      <td>42.57688</td>\n",
       "      <td>1.66769</td>\n",
       "      <td>AD</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El Tarter</td>\n",
       "      <td>&lt;El Tarter&gt;###################################...</td>\n",
       "      <td>42.57952</td>\n",
       "      <td>1.65362</td>\n",
       "      <td>AD</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sant Julià de Lòria</td>\n",
       "      <td>&lt;Sant Julia de Loria&gt;#########################...</td>\n",
       "      <td>42.46372</td>\n",
       "      <td>1.49129</td>\n",
       "      <td>AD</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pas de la Casa</td>\n",
       "      <td>&lt;Pas de la Casa&gt;##############################...</td>\n",
       "      <td>42.54277</td>\n",
       "      <td>1.73361</td>\n",
       "      <td>AD</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ordino</td>\n",
       "      <td>&lt;Ordino&gt;######################################...</td>\n",
       "      <td>42.55623</td>\n",
       "      <td>1.53319</td>\n",
       "      <td>AD</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name                                          asciiname  \\\n",
       "0               Soldeu  <Soldeu>######################################...   \n",
       "1            El Tarter  <El Tarter>###################################...   \n",
       "2  Sant Julià de Lòria  <Sant Julia de Loria>#########################...   \n",
       "3       Pas de la Casa  <Pas de la Casa>##############################...   \n",
       "4               Ordino  <Ordino>######################################...   \n",
       "\n",
       "   latitude  longitude country_code  target_len  \n",
       "0  42.57688    1.66769           AD           8  \n",
       "1  42.57952    1.65362           AD          11  \n",
       "2  42.46372    1.49129           AD          21  \n",
       "3  42.54277    1.73361           AD          16  \n",
       "4  42.55623    1.53319           AD           8  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "df[TARGET_COL] = START_CHAR + df[TARGET_COL] + END_CHAR\n",
    "df[\"target_len\"] = df[TARGET_COL].apply(len)\n",
    "\n",
    "max_len = max([len(city) for city in df[TARGET_COL].values])\n",
    "df[TARGET_COL] = df[TARGET_COL].str.pad(max_len, side=\"right\", fillchar=PADDING_CHAR)\n",
    "\n",
    "chars = sorted(list(set(\"\".join(df[TARGET_COL].values))))\n",
    "vocab_len = len(chars)\n",
    "print(\"Vocabulary length:\", vocab_len)\n",
    "print(\"Vocabulary:\", \"\".join(chars))\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [6, 26, 49, 46, 38, 39, 55, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded: <Soldeu>##################################################\n"
     ]
    }
   ],
   "source": [
    "# Generate a mapping from character to index and vice versa\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "encode = lambda x: [char_to_idx[char] for char in x]\n",
    "decode = lambda x: \"\".join([idx_to_char[idx] for idx in x])\n",
    "\n",
    "test_sample = df[TARGET_COL].values[0]\n",
    "print(\"Encoded:\", encode(test_sample))\n",
    "print(\"Decoded:\", decode(encode(test_sample)))\n",
    "\n",
    "START_TOKEN = encode(START_CHAR)[0]\n",
    "END_TOKEN = encode(END_CHAR)[0]\n",
    "PADDING_TOKEN = encode(PADDING_CHAR)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>asciiname</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>country_code</th>\n",
       "      <th>target_len</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Soldeu</td>\n",
       "      <td>&lt;Soldeu&gt;######################################...</td>\n",
       "      <td>42.57688</td>\n",
       "      <td>1.66769</td>\n",
       "      <td>AD</td>\n",
       "      <td>8</td>\n",
       "      <td>[6, 26, 49, 46, 38, 39, 55, 7, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El Tarter</td>\n",
       "      <td>&lt;El Tarter&gt;###################################...</td>\n",
       "      <td>42.57952</td>\n",
       "      <td>1.65362</td>\n",
       "      <td>AD</td>\n",
       "      <td>11</td>\n",
       "      <td>[6, 12, 46, 0, 27, 35, 52, 54, 39, 52, 7, 1, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name                                          asciiname  latitude  \\\n",
       "0     Soldeu  <Soldeu>######################################...  42.57688   \n",
       "1  El Tarter  <El Tarter>###################################...  42.57952   \n",
       "\n",
       "   longitude country_code  target_len  \\\n",
       "0    1.66769           AD           8   \n",
       "1    1.65362           AD          11   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [6, 26, 49, 46, 38, 39, 55, 7, 1, 1, 1, 1, 1, ...  \n",
       "1  [6, 12, 46, 0, 27, 35, 52, 54, 39, 52, 7, 1, 1...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokenized\"] = df[TARGET_COL].apply(encode)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 173012\n"
     ]
    }
   ],
   "source": [
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "val_df = df.drop(train_df.index)\n",
    "print(\"Train size:\", len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[52, 56, 43, 48, 49,  7,  1,  1],\n",
       "        [39, 52, 52, 59,  7,  1,  1,  1],\n",
       "        [35, 48, 38,  7,  1,  1,  1,  1],\n",
       "        [39, 52, 60, 35, 41, 35,  7,  1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[56, 43, 48, 49,  7,  1,  1,  1],\n",
       "        [52, 52, 59,  7,  1,  1,  1,  1],\n",
       "        [48, 38,  7,  1,  1,  1,  1,  1],\n",
       "        [52, 60, 35, 41, 35,  7,  1,  1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_batch(split, batch_size=BATCH_SIZE):\n",
    "    if split == \"train\":\n",
    "        df = train_df\n",
    "    elif split == \"val\":\n",
    "        df = val_df\n",
    "    x = []\n",
    "    y = []\n",
    "    sample_idx = torch.randint(0, len(df), (batch_size,))\n",
    "    for sidx in sample_idx:\n",
    "        target_len = df.iloc[int(sidx)][\"target_len\"]\n",
    "        idx = torch.randint(0, target_len - 1, (1,)).int()\n",
    "        \n",
    "        x_tensor = torch.tensor(df.iloc[int(sidx)][\"tokenized\"][idx:idx+BLOCK_SIZE])\n",
    "        y_tensor = torch.tensor(df.iloc[int(sidx)][\"tokenized\"][idx+1:idx+BLOCK_SIZE+1])\n",
    "        x.append(x_tensor)\n",
    "        y.append(y_tensor)\n",
    "        \n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\", 4)\n",
    "\n",
    "display(xb)\n",
    "display(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.412733 M parameters\n",
      "<m>\n"
     ]
    }
   ],
   "source": [
    "class MaskedTensor:\n",
    "    def __init__(self, tensor, mask):\n",
    "        self.tensor = tensor\n",
    "        self.mask = mask\n",
    "\n",
    "\n",
    "class CausalSelfAttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(N_EMBED, head_size, bias=False)\n",
    "        self.key = nn.Linear(N_EMBED, head_size, bias=False)\n",
    "        self.value = nn.Linear(N_EMBED, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        B, T, C = x.shape # (batch_size, seq_len, n_embed)\n",
    "\n",
    "        query = self.query(x) # (B, T, C)\n",
    "        key = self.key(x) # (B, T, C)\n",
    "\n",
    "        # Compute attention scores:\n",
    "        wei = query @ key.transpose(-2, -1) * C**(-0.5) # (B, T, C) @ (B, C, T) = (B, T, T)\n",
    "        # Mask out future tokens:\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # (B, T, T)\n",
    "        # Mask out padding tokens:\n",
    "        if padding_mask is not None:\n",
    "           wei = wei.masked_fill(padding_mask.unsqueeze(1).expand(-1, T, -1), float(\"-1e9\"))\n",
    "        \n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        return wei @ v # (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CausalSelfAttentionHead(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(N_EMBED, N_EMBED)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        out = torch.cat([h(x, padding_mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(DROPOUT),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x: MaskedTensor):\n",
    "        x.tensor = self.sa(self.ln1(x.tensor), x.mask)\n",
    "        x.tensor = self.ffwd(self.ln2(x.tensor))\n",
    "        return x\n",
    "\n",
    "class WherePT(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_len, N_EMBED)\n",
    "        self.position_embedding = nn.Embedding(BLOCK_SIZE, N_EMBED)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(N_EMBED, N_HEADS) for _ in range(N_LAYER)])\n",
    "        self.ln_final = nn.LayerNorm(N_EMBED)\n",
    "        self.lm_head = nn.Linear(N_EMBED, vocab_len)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        padding_mask = (idx == PADDING_TOKEN) # (batch_size, seq_len)\n",
    "\n",
    "        tok_embeds = self.token_embedding(idx) # (batch_size, seq_len, n_embed)\n",
    "        pos_embeds = self.position_embedding(torch.arange(T)) # (seq_len, n_embed)\n",
    "        x = MaskedTensor(tok_embeds + pos_embeds, padding_mask) # (batch_size, seq_len, n_embed)\n",
    "        x = self.blocks(x)\n",
    "        x.tensor = self.ln_final(x.tensor)\n",
    "        logits = self.lm_head(x.tensor) # (batch_size, seq_len, vocab_len)  \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_len), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens=10):\n",
    "        # idx: (batch_size, seq_len)\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -BLOCK_SIZE:] # (batch_size, block_size)\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "            if next_token == END_TOKEN:\n",
    "                break\n",
    "        return idx\n",
    "    \n",
    "\n",
    "model = WherePT()\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, \"M parameters\")\n",
    "\n",
    "idx = torch.tensor([START_TOKEN]).unsqueeze(0)\n",
    "output = model.generate(idx, max_len)[0].tolist()\n",
    "print(decode(output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'n_embed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/lage/Development/woher/gen3/playground.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lage/Development/woher/gen3/playground.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m wherept_config \u001b[39m=\u001b[39m wherept\u001b[39m.\u001b[39mWherePTConfig(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lage/Development/woher/gen3/playground.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m61\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lage/Development/woher/gen3/playground.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     N_EMBED,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lage/Development/woher/gen3/playground.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     DROPOUT\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lage/Development/woher/gen3/playground.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lage/Development/woher/gen3/playground.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m wherept\u001b[39m.\u001b[39;49mWherePT(wherept_config)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lage/Development/woher/gen3/playground.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\n",
      "File \u001b[0;32m~/Development/woher/gen3/wherept.py:106\u001b[0m, in \u001b[0;36mWherePT.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_len, config\u001b[39m.\u001b[39mn_embed)\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mblock_size, config\u001b[39m.\u001b[39mn_embed)\n\u001b[0;32m--> 106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39m[TransformerBlock(config) \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(config\u001b[39m.\u001b[39;49mn_layer)])\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(config\u001b[39m.\u001b[39mn_embed)\n\u001b[1;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mn_embed, config\u001b[39m.\u001b[39mvocab_len)\n",
      "File \u001b[0;32m~/Development/woher/gen3/wherept.py:106\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_len, config\u001b[39m.\u001b[39mn_embed)\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mblock_size, config\u001b[39m.\u001b[39mn_embed)\n\u001b[0;32m--> 106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39m[TransformerBlock(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mn_layer)])\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(config\u001b[39m.\u001b[39mn_embed)\n\u001b[1;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mn_embed, config\u001b[39m.\u001b[39mvocab_len)\n",
      "File \u001b[0;32m~/Development/woher/gen3/wherept.py:90\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     88\u001b[0m head_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mn_embed \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m config\u001b[39m.\u001b[39mn_head\n\u001b[1;32m     89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msa \u001b[39m=\u001b[39m MultiHeadAttention(config, head_size)\n\u001b[0;32m---> 90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffwd \u001b[39m=\u001b[39m FeedForward(config\u001b[39m.\u001b[39;49mn_embed)\n\u001b[1;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(config\u001b[39m.\u001b[39mn_embed)\n\u001b[1;32m     92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(config\u001b[39m.\u001b[39mn_embed)\n",
      "File \u001b[0;32m~/Development/woher/gen3/wherept.py:75\u001b[0m, in \u001b[0;36mFeedForward.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[1;32m     73\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     74\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m---> 75\u001b[0m         nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39;49mn_embed, \u001b[39m4\u001b[39m \u001b[39m*\u001b[39m config\u001b[39m.\u001b[39mn_embed),\n\u001b[1;32m     76\u001b[0m         nn\u001b[39m.\u001b[39mReLU(),\n\u001b[1;32m     77\u001b[0m         nn\u001b[39m.\u001b[39mLinear(\u001b[39m4\u001b[39m \u001b[39m*\u001b[39m config\u001b[39m.\u001b[39mn_embed, config\u001b[39m.\u001b[39mn_embed),\n\u001b[1;32m     78\u001b[0m         nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mdropout),\n\u001b[1;32m     79\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'n_embed'"
     ]
    }
   ],
   "source": [
    "wherept_config = wherept.WherePTConfig(\n",
    "    61,\n",
    "    N_EMBED,\n",
    "    N_HEADS,\n",
    "    N_LAYER,\n",
    "    BLOCK_SIZE,\n",
    "    DROPOUT\n",
    ")\n",
    "model = wherept.WherePT(wherept_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_ITERS = 10\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.8173, val loss 1.7555\n",
      "step 10: train loss 1.7403, val loss 1.8589\n",
      "step 20: train loss 1.8814, val loss 1.8569\n",
      "step 30: train loss 1.9221, val loss 1.9322\n",
      "step 40: train loss 1.9767, val loss 1.7983\n",
      "step 50: train loss 1.8991, val loss 1.9242\n",
      "step 60: train loss 1.9432, val loss 1.8237\n",
      "step 70: train loss 1.9388, val loss 1.8126\n",
      "step 80: train loss 1.9666, val loss 1.9387\n",
      "step 90: train loss 1.9629, val loss 1.9853\n",
      "step 100: train loss 1.6688, val loss 1.8583\n",
      "step 110: train loss 1.8294, val loss 1.7157\n",
      "step 120: train loss 1.8661, val loss 1.7022\n",
      "step 130: train loss 1.8663, val loss 1.7875\n",
      "step 140: train loss 1.9394, val loss 1.9591\n",
      "step 150: train loss 1.7888, val loss 1.8217\n",
      "step 160: train loss 1.7633, val loss 1.8663\n",
      "step 170: train loss 1.8800, val loss 1.9446\n",
      "step 180: train loss 1.8300, val loss 1.8497\n",
      "step 190: train loss 1.8393, val loss 1.8635\n",
      "step 200: train loss 1.8111, val loss 1.6579\n",
      "step 210: train loss 1.8804, val loss 1.7635\n",
      "step 220: train loss 1.9297, val loss 1.8188\n",
      "step 230: train loss 1.8524, val loss 1.6569\n",
      "step 240: train loss 1.8967, val loss 1.7816\n",
      "step 250: train loss 1.7717, val loss 1.7661\n",
      "step 260: train loss 1.8031, val loss 1.8175\n",
      "step 270: train loss 1.8827, val loss 1.8517\n",
      "step 280: train loss 1.8157, val loss 1.8555\n",
      "step 290: train loss 1.7623, val loss 1.7482\n",
      "step 300: train loss 1.7225, val loss 1.9426\n",
      "step 310: train loss 1.6953, val loss 1.7683\n",
      "step 320: train loss 1.9402, val loss 1.8069\n",
      "step 330: train loss 1.7883, val loss 1.7832\n",
      "step 340: train loss 1.7261, val loss 1.7394\n",
      "step 350: train loss 1.8485, val loss 1.8448\n",
      "step 360: train loss 1.8691, val loss 1.8824\n",
      "step 370: train loss 1.7132, val loss 1.8434\n",
      "step 380: train loss 1.8444, val loss 1.8101\n",
      "step 390: train loss 1.7861, val loss 1.9231\n",
      "step 400: train loss 1.7193, val loss 1.7193\n",
      "step 410: train loss 1.8554, val loss 1.8979\n",
      "step 420: train loss 1.8469, val loss 1.9173\n",
      "step 430: train loss 1.7210, val loss 1.7407\n",
      "step 440: train loss 1.7619, val loss 1.6869\n",
      "step 450: train loss 1.8307, val loss 1.7361\n",
      "step 460: train loss 1.7171, val loss 1.8802\n",
      "step 470: train loss 1.8116, val loss 1.7247\n",
      "step 480: train loss 1.8929, val loss 1.8569\n",
      "step 490: train loss 1.7348, val loss 1.8451\n",
      "step 500: train loss 1.7705, val loss 1.7889\n",
      "step 510: train loss 1.8066, val loss 1.7895\n",
      "step 520: train loss 1.7764, val loss 1.8194\n",
      "step 530: train loss 1.8909, val loss 1.8683\n",
      "step 540: train loss 1.8474, val loss 1.8622\n",
      "step 550: train loss 1.8411, val loss 1.7534\n",
      "step 560: train loss 1.8348, val loss 1.7835\n",
      "step 570: train loss 1.8814, val loss 1.7822\n",
      "step 580: train loss 1.6835, val loss 1.7638\n",
      "step 590: train loss 2.0508, val loss 1.8466\n",
      "step 600: train loss 1.8565, val loss 1.8813\n",
      "step 610: train loss 1.7823, val loss 1.8356\n",
      "step 620: train loss 1.8431, val loss 1.6690\n",
      "step 630: train loss 1.7407, val loss 1.8272\n",
      "step 640: train loss 1.6734, val loss 1.7766\n",
      "step 650: train loss 1.8281, val loss 1.8247\n",
      "step 660: train loss 1.7569, val loss 1.8345\n",
      "step 670: train loss 1.7034, val loss 1.9155\n",
      "step 680: train loss 1.7998, val loss 1.6422\n",
      "step 690: train loss 1.8510, val loss 1.6564\n",
      "step 700: train loss 1.8670, val loss 1.5979\n",
      "step 710: train loss 1.7221, val loss 1.7883\n",
      "step 720: train loss 1.8633, val loss 1.7157\n",
      "step 730: train loss 1.6276, val loss 1.7642\n",
      "step 740: train loss 1.7708, val loss 1.7399\n",
      "step 750: train loss 1.9237, val loss 1.7937\n",
      "step 760: train loss 1.7765, val loss 1.7792\n",
      "step 770: train loss 1.7390, val loss 1.8022\n",
      "step 780: train loss 1.7809, val loss 1.5992\n",
      "step 790: train loss 1.8384, val loss 1.8677\n",
      "step 800: train loss 1.7697, val loss 1.8946\n",
      "step 810: train loss 1.7915, val loss 1.6488\n",
      "step 820: train loss 1.8674, val loss 1.8255\n",
      "step 830: train loss 1.7825, val loss 1.8450\n",
      "step 840: train loss 1.8044, val loss 1.8394\n",
      "step 850: train loss 1.7361, val loss 1.8310\n",
      "step 860: train loss 1.7648, val loss 1.7588\n",
      "step 870: train loss 1.7601, val loss 1.6861\n",
      "step 880: train loss 1.8507, val loss 1.8375\n",
      "step 890: train loss 1.9494, val loss 1.8264\n",
      "step 900: train loss 1.8274, val loss 1.8246\n",
      "step 910: train loss 1.7777, val loss 1.8030\n",
      "step 920: train loss 1.8836, val loss 1.6193\n",
      "step 930: train loss 1.6761, val loss 1.8307\n",
      "step 940: train loss 1.7557, val loss 1.9497\n",
      "step 950: train loss 1.6785, val loss 1.7000\n",
      "step 960: train loss 1.8810, val loss 1.7507\n",
      "step 970: train loss 1.8624, val loss 1.7684\n",
      "step 980: train loss 1.8905, val loss 1.6816\n",
      "step 990: train loss 1.7834, val loss 1.8819\n",
      "step 999: train loss 1.7678, val loss 1.7009\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERS = 1000\n",
    "for iter in range(MAX_ITERS):\n",
    "    if iter % EVAL_ITERS == 0 or iter == MAX_ITERS - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    optimizer.zero_grad()\n",
    "    xb, yb = get_batch(\"train\", BATCH_SIZE)\n",
    "    logits, loss = model(xb, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Sicla Dhuuardlnnrecswosl>\n"
     ]
    }
   ],
   "source": [
    "idx = torch.tensor([START_TOKEN]).unsqueeze(0)\n",
    "#idx = torch.tensor(encode(\"<Stock\")).unsqueeze(0)\n",
    "\n",
    "output = model.generate(idx, max_len)[0].tolist()\n",
    "print(decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-feather-13</strong> at: <a href='https://wandb.ai/lage/woher/runs/9gnefhgt' target=\"_blank\">https://wandb.ai/lage/woher/runs/9gnefhgt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231202_151138-9gnefhgt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "woher-wqR2TfXF-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
